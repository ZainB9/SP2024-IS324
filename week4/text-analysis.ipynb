{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Text analysis is an essential part of social network analysis.\n",
    "> * Imagine you have a social media dataset with tweets. You want to analyze text to find nodes and edges as well as what they are talking about.\n",
    "> * You can use text analysis to find the most frequent words, hashtags, and mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TweetTokenizer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We are going to use the `nltk` library to analyze text.\n",
    "> * `from` is telling Python to import a library called `nltk`.\n",
    "> * `import` is used to import a specific module from a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('../week3/Political-media-DFE.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's import the data from week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
       "       '_last_judgment_at', 'audience', 'audience:confidence', 'bias',\n",
       "       'bias:confidence', 'message', 'message:confidence', 'orig__golden',\n",
       "       'audience_gold', 'bias_gold', 'bioid', 'embed', 'id', 'label',\n",
       "       'message_gold', 'source', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_unit_id                 int64\n",
       "_golden                   bool\n",
       "_unit_state             object\n",
       "_trusted_judgments       int64\n",
       "_last_judgment_at       object\n",
       "audience                object\n",
       "audience:confidence    float64\n",
       "bias                    object\n",
       "bias:confidence        float64\n",
       "message                 object\n",
       "message:confidence     float64\n",
       "orig__golden           float64\n",
       "audience_gold          float64\n",
       "bias_gold              float64\n",
       "bioid                   object\n",
       "embed                   object\n",
       "id                      object\n",
       "label                   object\n",
       "message_gold           float64\n",
       "source                  object\n",
       "text                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `.dtypes` is used to check the data type of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's subset the data to have who posted, where they posted (social media platform), and what they posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=data[['label', 'source', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>I applaud Governor PerryÛªs recent decision t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Today, I voted in favor of H.R. 5016 - Financi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>(Taken from posted WOKV interview)   Congressm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Join me next week for a town hall in Ocala! I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Foreign Affairs Committee Hearing on Syria. I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 label    source  \\\n",
       "0       From: Trey Radel (Representative from Florida)   twitter   \n",
       "1        From: Mitch McConnell (Senator from Kentucky)   twitter   \n",
       "2     From: Kurt Schrader (Representative from Oregon)   twitter   \n",
       "3             From: Michael Crapo (Senator from Idaho)   twitter   \n",
       "4             From: Mark Udall (Senator from Colorado)   twitter   \n",
       "...                                                ...       ...   \n",
       "4995      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4996      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4997      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4998      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4999      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "\n",
       "                                                   text  \n",
       "0     RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  \n",
       "1     VIDEO - #Obamacare:  Full of Higher Costs and ...  \n",
       "2     Please join me today in remembering our fallen...  \n",
       "3     RT @SenatorLeahy: 1st step toward Senate debat...  \n",
       "4     .@amazon delivery #drones show need to update ...  \n",
       "...                                                 ...  \n",
       "4995  I applaud Governor PerryÛªs recent decision t...  \n",
       "4996  Today, I voted in favor of H.R. 5016 - Financi...  \n",
       "4997  (Taken from posted WOKV interview)   Congressm...  \n",
       "4998  Join me next week for a town hall in Ocala! I'...  \n",
       "4999  Foreign Affairs Committee Hearing on Syria. I ...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We have contents from both twitter and facebook and have a text column for the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['twitter', 'facebook'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['source'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's print out some of the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @nowthisnews: Rep. Trey Radel (R- #FL) slams #Obamacare. #politics https://t.co/zvywMG8yIH'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VIDEO - #Obamacare:  Full of Higher Costs and Broken Promises: http://t.co/dn3vzqIrWF'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please join me today in remembering our fallen heroes and honoring the men and women currently in military service for their sacrifices.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Q. What additional information do you see in the contents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```YOUR ANSWER HERE```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Yes, we see hashtags, retweets, mentions, and URLs.\n",
    "> * In the third row, we see an example of a content only with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * In text analysis, it is important to make the text clean (remove unnecessary words, symbols, etc.) and to make the text uniform (lowercase, no punctuation, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Lowercase all the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/2949361602.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['text-lower']=content['text'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "content['text-lower']=content['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please join me today in remembering our fallen heroes and honoring the men and women currently in military service for their sacrifices.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We can seperate the entire contents into tokens (words, hashtags, mentions, etc.).\n",
    "> * Seperating the contents into tokens is called tokenization.\n",
    "> * We can use the `word_tokenize` function from the `nltk` library to tokenize the contents.\n",
    "> * There is also a `TweetTokenizer` function in the `nltk` library that is specifically for tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `.apply` is used to apply a function to a column. You don't have to use a for loop to apply a function to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/2884168308.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['tokenized_unigrams']=content['text-lower'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "content['tokenized_unigrams']=content['text-lower'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterated_unigrams=[]\n",
    "for idx, row in content.iterrows():\n",
    "    iterated_unigrams.append(word_tokenize(row['text-lower']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/2132692687.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['iterated_unigrams']=iterated_unigrams\n"
     ]
    }
   ],
   "source": [
    "content['iterated_unigrams']=iterated_unigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The results of iterating through each row and applying the `word_tokenize` function is a list of lists are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[0,'iterated_unigrams'] == content.loc[0,'tokenized_unigrams']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * One token is called a unigram.\n",
    "> * We can try to find bigrams (two tokens) and trigrams (three tokens) as well. \n",
    "> * All of these tokens are called n-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/3861493539.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['tokenized_bigrams']=content['text-lower'].apply(TweetTokenizer().tokenize).apply(lambda x: list(ngrams(x, 2)))\n"
     ]
    }
   ],
   "source": [
    "#bigrams lambda function\n",
    "content['tokenized_bigrams']=content['text-lower'].apply(TweetTokenizer().tokenize).apply(lambda x: list(ngrams(x, 2)))\n",
    "#It first tokenizes the text using TweetTokenizer, then creates biagrams using ngrams function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram iteration\n",
    "iterated_bigrams=[]\n",
    "for idx, row in content.iterrows():\n",
    "    iterated_bigrams.append(list(ngrams(row['tokenized_unigrams'], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/277072661.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['iterated_bigrams']=iterated_bigrams\n"
     ]
    }
   ],
   "source": [
    "content['iterated_bigrams']=iterated_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(rt, @nowthisnews), (@nowthisnews, :), (:, re...\n",
       "1       [(video, -), (-, #obamacare), (#obamacare, :),...\n",
       "2       [(please, join), (join, me), (me, today), (tod...\n",
       "3       [(rt, @senatorleahy), (@senatorleahy, :), (:, ...\n",
       "4       [(., @amazon), (@amazon, delivery), (delivery,...\n",
       "                              ...                        \n",
       "4995    [(i, applaud), (applaud, governor), (governor,...\n",
       "4996    [(today, ,), (,, i), (i, voted), (voted, in), ...\n",
       "4997    [((, taken), (taken, from), (from, posted), (p...\n",
       "4998    [(join, me), (me, next), (next, week), (week, ...\n",
       "4999    [(foreign, affairs), (affairs, committee), (co...\n",
       "Name: tokenized_bigrams, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['tokenized_bigrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [rt, @, nowthisnews, :, rep., trey, radel, (, ...\n",
       "1       [video, -, #, obamacare, :, full, of, higher, ...\n",
       "2       [please, join, me, today, in, remembering, our...\n",
       "3       [rt, @, senatorleahy, :, 1st, step, toward, se...\n",
       "4       [., @, amazon, delivery, #, drones, show, need...\n",
       "                              ...                        \n",
       "4995    [i, applaud, governor, perryûªs, recent, deci...\n",
       "4996    [today, ,, i, voted, in, favor, of, h.r, ., 50...\n",
       "4997    [(, taken, from, posted, wokv, interview, ), c...\n",
       "4998    [join, me, next, week, for, a, town, hall, in,...\n",
       "4999    [foreign, affairs, committee, hearing, on, syr...\n",
       "Name: tokenized_unigrams, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['tokenized_unigrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7668),\n",
       " ('.', 6855),\n",
       " ('to', 5913),\n",
       " (',', 4927),\n",
       " (':', 3763),\n",
       " ('and', 3678),\n",
       " ('of', 3371),\n",
       " ('in', 2892),\n",
       " ('#', 2518),\n",
       " ('a', 2508)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['tokenized_unigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('of', 'the'), 766),\n",
       " (('\\x89', 'ûªs'), 684),\n",
       " (('in', 'the'), 547),\n",
       " ((',', 'and'), 547),\n",
       " (('on', 'the'), 416),\n",
       " (('to', 'the'), 398),\n",
       " (('.', 'i'), 328),\n",
       " (('for', 'the'), 307),\n",
       " (('\\x89', 'û'), 305),\n",
       " (('at', 'the'), 305)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['tokenized_bigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * However, we see frequent words include function words (e.g., the, and, is, etc.) and punctuation.\n",
    "> * We can remove function words and punctuation to find the content words (e.g., nouns, verbs, adjectives, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>text-lower</th>\n",
       "      <th>tokenized_unigrams</th>\n",
       "      <th>iterated_unigrams</th>\n",
       "      <th>tokenized_bigrams</th>\n",
       "      <th>iterated_bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "      <td>rt @nowthisnews: rep. trey radel (r- #fl) slam...</td>\n",
       "      <td>[rt, @, nowthisnews, :, rep., trey, radel, (, ...</td>\n",
       "      <td>[rt, @, nowthisnews, :, rep., trey, radel, (, ...</td>\n",
       "      <td>[(rt, @nowthisnews), (@nowthisnews, :), (:, re...</td>\n",
       "      <td>[(rt, @), (@, nowthisnews), (nowthisnews, :), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "      <td>video - #obamacare:  full of higher costs and ...</td>\n",
       "      <td>[video, -, #, obamacare, :, full, of, higher, ...</td>\n",
       "      <td>[video, -, #, obamacare, :, full, of, higher, ...</td>\n",
       "      <td>[(video, -), (-, #obamacare), (#obamacare, :),...</td>\n",
       "      <td>[(video, -), (-, #), (#, obamacare), (obamacar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            label   source  \\\n",
       "0  From: Trey Radel (Representative from Florida)  twitter   \n",
       "1   From: Mitch McConnell (Senator from Kentucky)  twitter   \n",
       "\n",
       "                                                text  \\\n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...   \n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...   \n",
       "\n",
       "                                          text-lower  \\\n",
       "0  rt @nowthisnews: rep. trey radel (r- #fl) slam...   \n",
       "1  video - #obamacare:  full of higher costs and ...   \n",
       "\n",
       "                                  tokenized_unigrams  \\\n",
       "0  [rt, @, nowthisnews, :, rep., trey, radel, (, ...   \n",
       "1  [video, -, #, obamacare, :, full, of, higher, ...   \n",
       "\n",
       "                                   iterated_unigrams  \\\n",
       "0  [rt, @, nowthisnews, :, rep., trey, radel, (, ...   \n",
       "1  [video, -, #, obamacare, :, full, of, higher, ...   \n",
       "\n",
       "                                   tokenized_bigrams  \\\n",
       "0  [(rt, @nowthisnews), (@nowthisnews, :), (:, re...   \n",
       "1  [(video, -), (-, #obamacare), (#obamacare, :),...   \n",
       "\n",
       "                                    iterated_bigrams  \n",
       "0  [(rt, @), (@, nowthisnews), (nowthisnews, :), ...  \n",
       "1  [(video, -), (-, #), (#, obamacare), (obamacar...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/398659723.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['stopword']=content['text-lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n"
     ]
    }
   ],
   "source": [
    "content['stopword']=content['text-lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       rt @nowthisnews: rep. trey radel (r- #fl) slam...\n",
       "1       video - #obamacare: full higher costs broken p...\n",
       "2       please join today remembering fallen heroes ho...\n",
       "3       rt @senatorleahy: 1st step toward senate debat...\n",
       "4       .@amazon delivery #drones show need update law...\n",
       "                              ...                        \n",
       "4995    applaud governor perryûªs recent decision dep...\n",
       "4996    today, voted favor h.r. 5016 - financial servi...\n",
       "4997    (taken posted wokv interview) congressman yoho...\n",
       "4998    join next week town hall ocala! i'll answer qu...\n",
       "4999    foreign affairs committee hearing syria. remai...\n",
       "Name: stopword, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['stopword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       rt @nowthisnews: rep. trey radel (r- #fl) slam...\n",
       "1       video - #obamacare:  full of higher costs and ...\n",
       "2       please join me today in remembering our fallen...\n",
       "3       rt @senatorleahy: 1st step toward senate debat...\n",
       "4       .@amazon delivery #drones show need to update ...\n",
       "                              ...                        \n",
       "4995    i applaud governor perryûªs recent decision t...\n",
       "4996    today, i voted in favor of h.r. 5016 - financi...\n",
       "4997    (taken from posted wokv interview)   congressm...\n",
       "4998    join me next week for a town hall in ocala! i'...\n",
       "4999    foreign affairs committee hearing on syria. i ...\n",
       "Name: text-lower, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We'll get unigrams for the text after we remove function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/445710354.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['stop_tokenized_unigrams']=content['stopword'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "content['stop_tokenized_unigrams']=content['stopword'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We'll get bigrams for the text after we remove function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/1938189223.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['stop_tokenized_bigrams']=content['stopword'].apply(TweetTokenizer().tokenize).apply(lambda x: list(ngrams(x, 2)))\n"
     ]
    }
   ],
   "source": [
    "content['stop_tokenized_bigrams']=content['stopword'].apply(TweetTokenizer().tokenize).apply(lambda x: list(ngrams(x, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Here are the most frequent unigrams and bigrams after removing function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 6861),\n",
       " (',', 4927),\n",
       " (':', 3763),\n",
       " ('#', 2518),\n",
       " ('http', 2162),\n",
       " ('@', 1877),\n",
       " ('!', 995),\n",
       " (\"'s\", 812),\n",
       " ('today', 784),\n",
       " ('&', 589)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['stop_tokenized_unigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('\\x89', 'ûªs'), 684),\n",
       " (('\\x89', 'û'), 305),\n",
       " (('û', '\\x9d'), 249),\n",
       " (('.', '\\x89'), 198),\n",
       " (('s', '.'), 193),\n",
       " (('u', '.'), 192),\n",
       " (('.', 's'), 190),\n",
       " (('here', ':'), 183),\n",
       " (('.', '\"'), 154),\n",
       " (('\\x89', 'ûò'), 149)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['stop_tokenized_bigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We still see irrelevant punctuations. Let's get rid of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/3655593217.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['punct_tokenized_unigrams']=content['stop_tokenized_unigrams'].apply(lambda x: [word for word in x if word.isalnum()])\n"
     ]
    }
   ],
   "source": [
    "content['punct_tokenized_unigrams']=content['stop_tokenized_unigrams'].apply(lambda x: [word for word in x if word.isalnum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_iterated_bigrams=[]\n",
    "for idx, row in content.iterrows():\n",
    "    punct_iterated_bigrams.append(list(ngrams(row['punct_tokenized_unigrams'], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/218431191.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['punct_tokenized_bigrams']=punct_iterated_bigrams\n"
     ]
    }
   ],
   "source": [
    "content['punct_tokenized_bigrams']=punct_iterated_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's count the most frequent unigrams and bigrams after removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http', 2162),\n",
       " ('today', 784),\n",
       " ('house', 435),\n",
       " ('amp', 431),\n",
       " ('great', 396),\n",
       " ('new', 361),\n",
       " ('bill', 324),\n",
       " ('president', 317),\n",
       " ('act', 294),\n",
       " ('congress', 289)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['punct_tokenized_unigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('here', 'http'), 159),\n",
       " (('health', 'care'), 97),\n",
       " (('president', 'obama'), 83),\n",
       " (('united', 'states'), 75),\n",
       " (('town', 'hall'), 67),\n",
       " (('high', 'school'), 66),\n",
       " (('immigration', 'reform'), 61),\n",
       " (('small', 'business'), 47),\n",
       " (('last', 'night'), 46),\n",
       " (('make', 'sure'), 46)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['punct_tokenized_bigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * But we want to do additional cleaning. \n",
    "> * When counting the most frequent words, past and present tense of the same word are counted as different words.\n",
    "> * For example, \"run\" and \"running\" are counted as different words.\n",
    "> * We can use lemmatization to convert words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer() #Initialize lemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/3910947984.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['lemma']=content['punct_tokenized_unigrams'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
     ]
    }
   ],
   "source": [
    "content['lemma']=content['punct_tokenized_unigrams'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/816766601.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['lemma_str']=content['lemma'].apply(lambda x: ' '.join(x))\n"
     ]
    }
   ],
   "source": [
    "content['lemma_str']=content['lemma'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's compare the results of lemmatization and without lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'called usdotfra release info inspection casseltonderailment review quality rail'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[5, 'lemma_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'called on the @usdotfra to release info about inspections before the #casseltonderailment to review quality of rails. (1/2)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[5, 'text-lower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Q. Do you see any different results? What tokens have changed after lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YOUR ANSWER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better', pos=wordnet.ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cars'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('cars', pos=wordnet.VERB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Interestingly enough, NLTK's WordNetLemmatizer is not perfect.\n",
    "> * By default, it only lemmatize nouns.\n",
    "> * Therefore, we need to specify the part of speech (POS) for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): #ADJECTIVE\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'): #VERN\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'): #NOUN        \n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'): #ADVERB\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    # Tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged) \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # If no tag was found, then use the word as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # Else use the tag to lemmatize the word\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_4946/1287760357.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['lemmatizer_str']=content['lemma'].apply(lambda x: lemmatize_sentence(' '.join(x)))\n"
     ]
    }
   ],
   "source": [
    "content['lemmatizer_str']=content['lemma'].apply(lambda x: lemmatize_sentence(' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'called usdotfra release info inspection casseltonderailment review quality rail'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[5, 'lemma_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call usdotfra release info inspection casseltonderailment review quality rail'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[5, 'lemmatizer_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's extract hashtags and mentions from the contents.\n",
    "> * We have to use regular expressions to extract hashtags and mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * There are four useful regex functions in Python: `findall`, `search`, `split`, and `sub`.\n",
    "\n",
    "> * `findall` is used to find all matches of a pattern in a string.\n",
    "> * `search` is used to find the first match of a pattern in a string.\n",
    "> * `split` is used to split a string by a pattern.\n",
    "> * `sub` is used to replace a pattern in a string.\n",
    "\n",
    "> * In this class, we are going to use `findall` to extract hashtags and mentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The result of `findall` is a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Basic regex patterns:\n",
    "> * `.` matches any character except a newline.\n",
    "> * `*` matches 0 or more repetitions of the preceding regex pattern.\n",
    "> * `+` matches 1 or more repetitions of the preceding regex pattern.\n",
    "> * `?` matches 0 or 1 repetition of the preceding regex pattern.\n",
    "> * `^` matches the start of a string.\n",
    "> * `$` matches the end of a string.\n",
    "> * `[]` matches any one of the characters inside the square brackets.\n",
    "> * `\\` is used to escape special characters.\n",
    "> * `|` is used to match either the regex pattern on the left or the right.\n",
    "\n",
    "> * `[a-z]` matches any lowercase letter.\n",
    "> * `[A-Z]` matches any uppercase letter.\n",
    "> * `[0-9]` matches any digit.\n",
    "> * `\\d` matches any digit.\n",
    "> * `\\D` matches any non-digit.\n",
    "> * `\\w` matches any word character (alphanumeric and underscore).\n",
    "> * `\\W` matches any non-word character.\n",
    "> * `\\s` matches any whitespace character.\n",
    "> * `\\S` matches any non-whitespace character.\n",
    "\n",
    "> * `[a-zA-Z]` matches any alphabet character.\n",
    "> * `[a-zA-Z0-9]` matches any alphanumeric character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../week4/Regex-Cheat-Sheet.png\" width=500px height=800px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '3']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'\\d+') # \\d+ is the pattern to search for, it means any digit 0-9 and the + means one or more times\n",
    "sample1='I have 2 dogs and 3 cats'\n",
    "result=re.findall(pattern, sample1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is324socialnetworkanalysis@illinois.edu', 'jaihyunpark@illinois.edu']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}') \n",
    "#Matches one or more alphanumeric characters, dots, underscores, percent signs, plus signs, or hyphens. \n",
    "#The @ symbol is also matched.\n",
    "sample2='My primary email is is324socialnetworkanalysis@illinois.edu. Please contact me! If I am not replying, you can contact me at jaihyunpark@illinois.edu'\n",
    "result=re.findall(pattern, sample2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://is324.com']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'https?://\\S+')\n",
    "#Matches http or https, followed by ://, followed by any non-whitespace characters.\n",
    "sample3='Look at what is happening at Washington! https://is324.com'\n",
    "result=re.findall(pattern, sample3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#IS324']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'#[a-zA-Z0-9]+')\n",
    "#Matches hashtags that start with #, followed by one or more alphanumeric characters.\n",
    "sample4='I am so excited for the class #IS324'\n",
    "result=re.findall(pattern, sample4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@jaihyunpark']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'@[a-zA-Z0-9]+')\n",
    "#Matches mentions that start with @, followed by one or more alphanumeric characters.\n",
    "sample5='@jaihyunpark is the instructor for #IS324'\n",
    "result=re.findall(pattern, sample5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' @jaihyunpark', ' @socialmedia']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'(rt\\s+@[a-zA-Z0-9]+ | @[a-zA-Z0-9]+)')\n",
    "\n",
    "sample6='rt @jaihyunpark: I am so excited for the class #IS324 @socialmedia'\n",
    "result=re.findall(pattern, sample6)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's extract hashtags and mentions from the content DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       rt @nowthisnews: rep. trey radel (r- #fl) slam...\n",
       "1       video - #obamacare:  full of higher costs and ...\n",
       "2       please join me today in remembering our fallen...\n",
       "3       rt @senatorleahy: 1st step toward senate debat...\n",
       "4       .@amazon delivery #drones show need to update ...\n",
       "                              ...                        \n",
       "4995    i applaud governor perryûªs recent decision t...\n",
       "4996    today, i voted in favor of h.r. 5016 - financi...\n",
       "4997    (taken from posted wokv interview)   congressm...\n",
       "4998    join me next week for a town hall in ocala! i'...\n",
       "4999    foreign affairs committee hearing on syria. i ...\n",
       "Name: text-lower, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
